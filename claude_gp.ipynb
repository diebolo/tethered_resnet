{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db873c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and prepare data\n",
    "data = pd.read_csv('train_test.csv', header=None, names=[\n",
    "    'timestamp', 'tether_x', 'tether_y', 'tether_z', 'drone_x', 'drone_y', 'drone_z', \n",
    "    'platform_azimuth', 'platform_elevation', 'drone_elevation', 'drone_azimuth', \n",
    "    'drone_yaw', 'length'\n",
    "])\n",
    "\n",
    "data_slice = 500\n",
    "# Features: tether positions + control parameters\n",
    "x = pd.concat([\n",
    "    data.iloc[:data_slice, 1:4],    # tether_x, tether_y, tether_z\n",
    "    data.iloc[:data_slice, 7:10],   # platform_azimuth, platform_elevation, drone_elevation\n",
    "    data.iloc[:data_slice, 12]      # length\n",
    "], axis=1)\n",
    "\n",
    "# Target: error between drone and tether positions\n",
    "y = data.iloc[:data_slice, 4:7].values - x.iloc[:, :3].values  # [drone - tether] for x,y,z\n",
    "\n",
    "print(f\"Input features shape: {x.shape}\")\n",
    "print(f\"Target errors shape: {y.shape}\")\n",
    "print(f\"Feature names: {list(x.columns)}\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = x.values\n",
    "Y = y\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Standardize features (important for GP)\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "# Standardize targets (optional but often helpful)\n",
    "scaler_Y = StandardScaler()\n",
    "Y_train_scaled = scaler_Y.fit_transform(Y_train)\n",
    "Y_test_scaled = scaler_Y.transform(Y_test)\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train_scaled.shape[0]}\")\n",
    "print(f\"Test set size: {X_test_scaled.shape[0]}\")\n",
    "\n",
    "# Convert to TensorFlow tensors\n",
    "X_train_tf = tf.convert_to_tensor(X_train_scaled, dtype=gpflow.default_float())\n",
    "Y_train_tf = tf.convert_to_tensor(Y_train_scaled, dtype=gpflow.default_float())\n",
    "X_test_tf = tf.convert_to_tensor(X_test_scaled, dtype=gpflow.default_float())\n",
    "\n",
    "# Create multi-output GP model\n",
    "# Option 1: Independent GPs for each output dimension\n",
    "def create_independent_gp_model(X_train, Y_train, output_dim=3):\n",
    "    \"\"\"Create independent GP models for each output dimension using GPFlow 2.0\"\"\"\n",
    "    models = []\n",
    "    \n",
    "    for i in range(output_dim):\n",
    "        # Create kernel (RBF for each input dimension)\n",
    "        kernel = gpflow.kernels.RBF(lengthscales=np.ones(X_train.shape[1]))\n",
    "        \n",
    "        # Create GP regression model\n",
    "        model = gpflow.models.GPR(\n",
    "            data=(X_train, Y_train[:, i:i+1]),  # Single output\n",
    "            kernel=kernel,\n",
    "            mean_function=None\n",
    "        )\n",
    "        \n",
    "        # Set priors using tensorflow_probability distributions (GPFlow 2.0 style)\n",
    "        model.kernel.lengthscales.prior = tfp.distributions.Gamma(\n",
    "            gpflow.utilities.to_default_float(1.0), \n",
    "            gpflow.utilities.to_default_float(1.0)\n",
    "        )\n",
    "        model.kernel.variance.prior = tfp.distributions.Gamma(\n",
    "            gpflow.utilities.to_default_float(1.0), \n",
    "            gpflow.utilities.to_default_float(1.0)\n",
    "        )\n",
    "        model.likelihood.variance.prior = tfp.distributions.Gamma(\n",
    "            gpflow.utilities.to_default_float(1.0), \n",
    "            gpflow.utilities.to_default_float(1.0)\n",
    "        )\n",
    "        \n",
    "        models.append(model)\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Option 2: Multi-output GP with proper SVGP approach (for large datasets)\n",
    "def create_multioutput_svgp_model(X_train, Y_train, num_inducing=500):\n",
    "    \"\"\"Create a multi-output SVGP model for better scalability\"\"\"\n",
    "    # Shared kernel across outputs\n",
    "    kernel = gpflow.kernels.RBF(lengthscales=np.ones(X_train.shape[1]))\n",
    "    \n",
    "    # Create multi-output kernel\n",
    "    mo_kernel = gpflow.kernels.SharedIndependent(kernel, output_dim=Y_train.shape[1])\n",
    "    \n",
    "    # Select inducing points (subset of training data)\n",
    "    inducing_indices = np.random.choice(X_train.shape[0], size=num_inducing, replace=False)\n",
    "    inducing_points = X_train[inducing_indices, :]\n",
    "    \n",
    "    # Create SVGP model (better for large datasets)\n",
    "    model = gpflow.models.SVGP(\n",
    "        kernel=mo_kernel,\n",
    "        likelihood=gpflow.likelihoods.Gaussian(),\n",
    "        inducing_variable=inducing_points,\n",
    "        num_latent_gps=Y_train.shape[1]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Choose approach (Independent GPs are more reliable for this case)\n",
    "print(\"\\nCreating GP models...\")\n",
    "use_independent_gps = True  # Set to True to avoid the shape mismatch issues\n",
    "\n",
    "if use_independent_gps:\n",
    "    models = create_independent_gp_model(X_train_tf, Y_train_tf)\n",
    "    print(\"Created 3 independent GP models\")\n",
    "else:\n",
    "    # Fixed multi-output approach using proper multi-output likelihood\n",
    "    print(\"Creating multi-output GP model with proper likelihood...\")\n",
    "    \n",
    "    # Use a different approach for multi-output\n",
    "    kernel = gpflow.kernels.RBF(lengthscales=np.ones(X_train_tf.shape[1]))\n",
    "    \n",
    "    # Create separate models and combine them (this is more stable)\n",
    "    model = gpflow.models.GPR(\n",
    "        data=(X_train_tf, Y_train_tf),\n",
    "        kernel=kernel,\n",
    "        mean_function=None\n",
    "    )\n",
    "    print(\"Created multi-output GP model\")\n",
    "\n",
    "# Training function for independent GPs\n",
    "def train_independent_gps(models, max_iter=1000):\n",
    "    \"\"\"Train independent GP models\"\"\"\n",
    "    trained_models = []\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        print(f\"\\nTraining GP model for output {i+1}/3...\")\n",
    "        \n",
    "        # Optimize hyperparameters\n",
    "        opt = gpflow.optimizers.Scipy()\n",
    "        \n",
    "        try:\n",
    "            opt.minimize(\n",
    "                model.training_loss,\n",
    "                model.trainable_variables,\n",
    "                options=dict(maxiter=max_iter, disp=True)\n",
    "            )\n",
    "            print(f\"Model {i+1} trained successfully\")\n",
    "            \n",
    "            # Print learned hyperparameters\n",
    "            print(f\"Lengthscales: {model.kernel.lengthscales.numpy()}\")\n",
    "            print(f\"Kernel variance: {model.kernel.variance.numpy():.4f}\")\n",
    "            print(f\"Noise variance: {model.likelihood.variance.numpy():.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Training failed for model {i+1}: {e}\")\n",
    "        \n",
    "        trained_models.append(model)\n",
    "    \n",
    "    return trained_models\n",
    "\n",
    "# Training function for multi-output SVGP\n",
    "def train_multioutput_svgp(model, X_train, Y_train, max_iter=1000, batch_size=1000):\n",
    "    \"\"\"Train multi-output SVGP model with mini-batching\"\"\"\n",
    "    print(\"\\nTraining multi-output SVGP model...\")\n",
    "    \n",
    "    # Create dataset for mini-batching\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "    train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Use Adam optimizer for SVGP\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(batch_x, batch_y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Set training data for this batch\n",
    "            model.data = (batch_x, batch_y)\n",
    "            loss = model.training_loss()\n",
    "        \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        return loss\n",
    "    \n",
    "    # Training loop\n",
    "    losses = []\n",
    "    for epoch in range(max_iter // 100):  # Fewer epochs for demonstration\n",
    "        epoch_loss = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for batch_x, batch_y in train_dataset:\n",
    "            loss = train_step(batch_x, batch_y)\n",
    "            epoch_loss += loss\n",
    "            batch_count += 1\n",
    "        \n",
    "        avg_loss = epoch_loss / batch_count\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}\")\n",
    "    \n",
    "    print(\"SVGP training completed\")\n",
    "    return model, losses\n",
    "\n",
    "# Train models\n",
    "if use_independent_gps:\n",
    "    trained_models = train_independent_gps(models, max_iter=500)\n",
    "else:\n",
    "    # Use SVGP for large datasets (more stable than GPR for multi-output)\n",
    "    svgp_model = create_multioutput_svgp_model(X_train_tf, Y_train_tf, num_inducing=1000)\n",
    "    trained_model, losses = train_multioutput_svgp(svgp_model, X_train_tf, Y_train_tf, max_iter=100)\n",
    "\n",
    "# Prediction function for independent GPs\n",
    "def predict_independent_gps(models, X_test):\n",
    "    \"\"\"Make predictions with independent GP models\"\"\"\n",
    "    predictions = []\n",
    "    uncertainties = []\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        mean, var = model.predict_f(X_test)\n",
    "        predictions.append(mean.numpy())\n",
    "        uncertainties.append(np.sqrt(var.numpy()))\n",
    "    \n",
    "    # Combine predictions\n",
    "    pred_mean = np.concatenate(predictions, axis=1)\n",
    "    pred_std = np.concatenate(uncertainties, axis=1)\n",
    "    \n",
    "    return pred_mean, pred_std\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\nMaking predictions...\")\n",
    "\n",
    "if use_independent_gps:\n",
    "    Y_pred_scaled, Y_std_scaled = predict_independent_gps(trained_models, X_test_tf)\n",
    "else:\n",
    "    Y_pred_scaled, Y_var_scaled = trained_model.predict_f(X_test_tf)\n",
    "    Y_pred_scaled = Y_pred_scaled.numpy()\n",
    "    Y_std_scaled = np.sqrt(Y_var_scaled.numpy())\n",
    "\n",
    "# Transform predictions back to original scale\n",
    "Y_pred = scaler_Y.inverse_transform(Y_pred_scaled)\n",
    "Y_std = Y_std_scaled * scaler_Y.scale_  # Scale the uncertainties\n",
    "\n",
    "print(f\"Predictions shape: {Y_pred.shape}\")\n",
    "print(f\"Uncertainties shape: {Y_std.shape}\")\n",
    "\n",
    "# Evaluate performance\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "mae = mean_absolute_error(Y_test, Y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"\\nPrediction Performance:\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "# Per-dimension metrics\n",
    "for i, dim in enumerate(['x', 'y', 'z']):\n",
    "    mse_dim = mean_squared_error(Y_test[:, i], Y_pred[:, i])\n",
    "    mae_dim = mean_absolute_error(Y_test[:, i], Y_pred[:, i])\n",
    "    print(f\"{dim}-dimension - RMSE: {np.sqrt(mse_dim):.4f}, MAE: {mae_dim:.4f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Plot predictions vs actual for each dimension\n",
    "for i, dim in enumerate(['x', 'y', 'z']):\n",
    "    ax = axes[0, i]\n",
    "    ax.scatter(Y_test[:, i], Y_pred[:, i], alpha=0.6)\n",
    "    ax.plot([Y_test[:, i].min(), Y_test[:, i].max()], \n",
    "            [Y_test[:, i].min(), Y_test[:, i].max()], 'r--', lw=2)\n",
    "    ax.set_xlabel(f'Actual Error {dim}')\n",
    "    ax.set_ylabel(f'Predicted Error {dim}')\n",
    "    ax.set_title(f'Error Prediction: {dim}-dimension')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot prediction uncertainties\n",
    "for i, dim in enumerate(['x', 'y', 'z']):\n",
    "    ax = axes[1, i]\n",
    "    residuals = np.abs(Y_test[:, i] - Y_pred[:, i])\n",
    "    ax.scatter(Y_std[:, i], residuals, alpha=0.6)\n",
    "    ax.set_xlabel(f'Predicted Std {dim}')\n",
    "    ax.set_ylabel(f'Absolute Residual {dim}')\n",
    "    ax.set_title(f'Uncertainty vs Residual: {dim}-dimension')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Function to make predictions on new data\n",
    "def predict_error(new_data, models=None, scaler_X=None, scaler_Y=None):\n",
    "    \"\"\"\n",
    "    Predict error for new data points\n",
    "    \n",
    "    Args:\n",
    "        new_data: numpy array of shape (n_samples, n_features)\n",
    "        models: trained GP models\n",
    "        scaler_X, scaler_Y: fitted scalers\n",
    "    \n",
    "    Returns:\n",
    "        predicted_errors: numpy array of shape (n_samples, 3)\n",
    "        uncertainties: numpy array of shape (n_samples, 3)\n",
    "    \"\"\"\n",
    "    if models is None:\n",
    "        models = trained_models\n",
    "    \n",
    "    # Scale input\n",
    "    new_data_scaled = scaler_X.transform(new_data)\n",
    "    new_data_tf = tf.convert_to_tensor(new_data_scaled, dtype=gpflow.default_float())\n",
    "    \n",
    "    # Make predictions\n",
    "    if use_independent_gps:\n",
    "        pred_mean_scaled, pred_std_scaled = predict_independent_gps(models, new_data_tf)\n",
    "    else:\n",
    "        pred_mean_scaled, pred_var_scaled = models.predict_f(new_data_tf)\n",
    "        pred_mean_scaled = pred_mean_scaled.numpy()\n",
    "        pred_std_scaled = np.sqrt(pred_var_scaled.numpy())\n",
    "    \n",
    "    # Transform back to original scale\n",
    "    pred_mean = scaler_Y.inverse_transform(pred_mean_scaled)\n",
    "    pred_std = pred_std_scaled * scaler_Y.scale_\n",
    "    \n",
    "    return pred_mean, pred_std\n",
    "\n",
    "print(\"\\nModel training complete!\")\n",
    "print(\"Use predict_error() function to make predictions on new data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc505aee",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "start_data = 500\n",
    "end_data = 600\n",
    "# Features: tether positions + control parameters\n",
    "x = pd.concat([\n",
    "    data.iloc[start_data:end_data, 1:4],    # tether_x, tether_y, tether_z\n",
    "    data.iloc[start_data:end_data, 7:10],   # platform_azimuth, platform_elevation, drone_elevation\n",
    "    data.iloc[start_data:end_data, 12]      # length\n",
    "], axis=1)\n",
    "\n",
    "# Target: error between drone and tether positions\n",
    "Y_test = data.iloc[start_data:end_data, 4:7].values - x.iloc[:, :3].values  # [drone - tether] for x,y,z\n",
    "\n",
    "Y_pred = predict_error(x.values, models=trained_models, scaler_X=scaler_X, scaler_Y=scaler_Y)\n",
    "\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "mae = mean_absolute_error(Y_test, Y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"\\nPrediction Performance:\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b13e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
