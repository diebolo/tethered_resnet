{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "364fb652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (67056, 4)\n",
      "Y_train shape: (67056, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import gpflow\n",
    "from gpflow.utilities import set_trainable\n",
    "from gpflow.kernels import SquaredExponential, White\n",
    "from gpflow.models import GPMC\n",
    "from gpflow.likelihoods import Gaussian\n",
    "from gpflow.kernels import SeparateIndependent, SharedIndependent\n",
    "# from gpflow.likelihoods import SeparateIndependentGaussian\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and prepare your data\n",
    "data = pd.read_csv('train_test.csv', header=None, names=['timestamp', 'tether_x', 'tether_y', 'tether_z', 'drone_x', 'drone_y', 'drone_z', 'platform_azimuth', 'platform_elevation', 'drone_elevation', 'drone_azimuth', 'drone_yaw', 'length'])\n",
    "\n",
    "# Inputs (X)\n",
    "# We need to choose the inputs that are most relevant.\n",
    "# The `tether_x,y,z` are already a function of the other parameters.\n",
    "# A good starting point is to use the raw sensor data that generates the tether position.\n",
    "X_inputs = data[['platform_azimuth', 'platform_elevation', 'drone_elevation', 'length']].values\n",
    "\n",
    "# It's often good practice to normalize the input data to have zero mean and unit variance\n",
    "# This helps with the optimization process.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_x = StandardScaler()\n",
    "X_norm = scaler_x.fit_transform(X_inputs)\n",
    "\n",
    "# Outputs (Y)\n",
    "y = data[['drone_x', 'drone_y', 'drone_z']].values - data[['tether_x', 'tether_y', 'tether_z']].values\n",
    "\n",
    "# For multi-output GPs, y should have shape [N, D], where N is number of data points and D is number of outputs.\n",
    "# Your current `y` is already in this format, which is perfect.\n",
    "\n",
    "# Convert to TensorFlow tensors with the right data type\n",
    "X = tf.convert_to_tensor(X_norm, dtype=tf.float64)\n",
    "Y = tf.convert_to_tensor(y, dtype=tf.float64)\n",
    "\n",
    "# Split data into training and testing sets (optional but good practice)\n",
    "# Let's use a simple split\n",
    "N_train = int(0.8 * len(X))\n",
    "X_train, Y_train = X[:N_train], Y[:N_train]\n",
    "X_test, Y_test = X[N_train:], Y[N_train:]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"Y_train shape: {Y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1b98f5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gpflow.likelihoods' has no attribute 'SeparateIndependent'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m likelihood \u001b[38;5;241m=\u001b[39m gpflow\u001b[38;5;241m.\u001b[39mlikelihoods\u001b[38;5;241m.\u001b[39mGaussian(variance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;66;03m# We'll need a multi-output likelihood here\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# For multi-output, it's often better to use a multi-output likelihood\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# This allows for a different observation noise variance for each output dimension.\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m multi_likelihood \u001b[38;5;241m=\u001b[39m \u001b[43mgpflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlikelihoods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeparateIndependent\u001b[49m(num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m     24\u001b[0m                                                         likelihoods\u001b[38;5;241m=\u001b[39m[gpflow\u001b[38;5;241m.\u001b[39mlikelihoods\u001b[38;5;241m.\u001b[39mGaussian() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_outputs)])\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Build the GP model\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# We'll use GPR (Gaussian Process Regression) as it's the most straightforward.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m gpflow\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mGPR(\n\u001b[1;32m     29\u001b[0m     (X_train, Y_train), \n\u001b[1;32m     30\u001b[0m     kernel\u001b[38;5;241m=\u001b[39mkernel, \n\u001b[1;32m     31\u001b[0m     likelihood\u001b[38;5;241m=\u001b[39mmulti_likelihood \u001b[38;5;66;03m# Using a multi-output likelihood\u001b[39;00m\n\u001b[1;32m     32\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'gpflow.likelihoods' has no attribute 'SeparateIndependent'"
     ]
    }
   ],
   "source": [
    "# Number of inputs and outputs\n",
    "num_outputs = Y.shape[1]  # 3 (for x, y, z errors)\n",
    "input_dim = X.shape[1]   # 4 (for p_azimuth, p_elevation, d_elevation, length)\n",
    "\n",
    "# A good kernel choice is a combination of a basic kernel (like RBF) and a White kernel for noise.\n",
    "# We will use a SharedIndependent kernel where one kernel is shared across all outputs.\n",
    "# This implies that the correlation structure is the same for all outputs, but each output\n",
    "# can have a different scaling.\n",
    "\n",
    "# Define the shared kernel (e.g., SquaredExponential)\n",
    "shared_kernel = gpflow.kernels.SquaredExponential(lengthscales=1.0, variance=1.0)\n",
    "shared_kernel.lengthscales.trainable = True # Ensure lengthscales are trainable\n",
    "\n",
    "# Build the multi-output kernel\n",
    "# `num_outputs` is the number of target variables (3: x, y, z errors)\n",
    "kernel = gpflow.kernels.SharedIndependent(shared_kernel, output_dim=num_outputs)\n",
    "\n",
    "# The likelihood will handle the observation noise for each output dimension\n",
    "likelihood = gpflow.likelihoods.Gaussian(variance=1.0) # We'll need a multi-output likelihood here\n",
    "\n",
    "# For multi-output, it's often better to use a multi-output likelihood\n",
    "# This allows for a different observation noise variance for each output dimension.\n",
    "multi_likelihood = gpflow.likelihoods.SeparateIndependent(num_outputs=num_outputs,\n",
    "                                                        likelihoods=[gpflow.likelihoods.Gaussian() for _ in range(num_outputs)])\n",
    "\n",
    "# Build the GP model\n",
    "# We'll use GPR (Gaussian Process Regression) as it's the most straightforward.\n",
    "model = gpflow.models.GPR(\n",
    "    (X_train, Y_train), \n",
    "    kernel=kernel, \n",
    "    likelihood=multi_likelihood # Using a multi-output likelihood\n",
    ")\n",
    "\n",
    "# You can inspect the model's parameters\n",
    "print(\"Initial model parameters:\")\n",
    "gpflow.utilities.print_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b4586e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
